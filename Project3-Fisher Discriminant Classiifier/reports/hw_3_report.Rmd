---
title: "STAT 760 - Homework 3"
author: "Alex Knudson"
output:
  pdf_document: default
  html_document: default
subtitle: Mean and Covariance Matrix
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment = NA,
                      warning = FALSE,
                      fig.width = 2,
                      fig.height = 2,
                      cache = TRUE)
pacman::p_load(tidyverse, xfun, kableExtra)

digits <- numbers_to_words(0:9)

train <- read.table("data/zip.train.gz") %>% as.matrix
```

# Function Definitions

```{r}
# Compute the mean of each variable in a data set
mu_mat <- function(x) {
  n <- nrow(x)
  n^(-1) * t(x) %*% matrix(1, n, 1)
}

# Compute the variance-covariance matrix of a data set
cov_mat <- function(x) {
  n <- nrow(x)
  x_bar <- mu_mat(x)
  n^-1 * t(x) %*% (diag(1,n) - n^-1 * matrix(1,n,1) %*% matrix(1,1,n)) %*% x
}

# Rotate 2D data by some angle (radians)
rotate <- function(x, theta) {
  m <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)),
              nrow = 2, byrow = T)
  t(m %*% t(x))
}

# Projection of X on w
proj <- function(X, w) {
  as.numeric(t(w) %*% w)^-1 * (X %*% w) %*% t(w)
}
```

```{r, echo=FALSE}
# Bisection method for finding roots
root_bisect <- function(fun, ..., lower, upper, tol = 1e-8, max_n = 100) {
  if (lower > upper) {
    a <- upper
    b <- lower
  } else {
    a <- lower
    b <- upper
  }
  
  for (i in 1:max_n) {
    FA <- fun(a, ...)
    p  <- a + (b - a) / 2
    FP <- fun(p, ...)
    if (FP == 0 | (b-a)/2 < tol) {
      return(p)
    }
    if (FA * FP > 0) {
      a  <- p
      FA <- FP
    } else {
      b <- p
    }
  }
  return(NA_real_)
}
```

\clearpage

# Problem 1

We are using the zip code data set from *The Elements of Statistical Learning* to compute the mean and covariance matrix for each digit, 0 through 9. After reading in the data, the class digits are split up into individual matrices.

```{r, echo = FALSE}
digit_mat <- vector("list", 10)  # Initialize a list to store matrix by digit
for (k in 0:9) {
  digit_mat[[k+1]] <- train[train[,1] == k, -1]
}

names(digit_mat) <- digits
```

**Compute the feature mean by digit** 

```{r, echo=FALSE}
mean_mat <- matrix(NA_real_, 256, 10, 
                   dimnames = list(
                     str_pad(as.character(1:256), 3, "left", "0"),
                     digits))
```

```{r}
for (k in digits) {
  mean_mat[,k] <- mu_mat(digit_mat[[k]])
}
```

**Report the feature mean by digit**

Output is truncated to the first and last 5 features of the 256 total.

```{r, echo=FALSE}
m <-5  # How many to print from the top and bottom

rbind(head(mean_mat, m), tail(mean_mat, m)) %>%
  kable(digits = 2) %>%
  kable_styling(latex_options = c("hold_position", "striped")) %>%
  add_header_above(c(" " = 1, "Handwritten Digit" = 10))
```

**Compute the covariance matrix of features for each digit**

Since the dimensions of each covariance matrix will be the same, I initialized a $[10 \times 256 \times 256]$ array, where each layer of the array will store the covariance matrix for each digit.

```{r, echo=FALSE}
cov_arr <- array(NA_real_, dim = c(10, 256, 256))
dimnames(cov_arr) <- list(digits, NULL, NULL)
```

```{r}
for (k in digits) {
  cov_arr[k,,] <- cov_mat(digit_mat[[k]])
}
```

**Compute the determinant for each covariance matrix**

```{r, echo=FALSE}
cov_det <- vector("numeric", 10)
names(cov_det) <- digits
```

```{r}
for (k in digits) {
  cov_det[k] <- det(cov_arr[k,,])
}
```

```{r, echo=FALSE}
cov_det %>% t() %>%
  kable(caption = "Determinant for each digit covariance matrix") %>%
  kable_styling(latex_options = "hold_position") %>%
  column_spec(1:10, width = "3em")
```

Since the determinant for each digit's covariance matrix is zero, we would need to use a method such as adding in random noise in order to take the inverse.

```{r cleanup_problem_1, echo=FALSE}
rm(digit_mat, mean_mat, train, cov_arr, cov_det, digits, k, m)
```

\clearpage

# Problem 2

For this problem, I generated two groups from a gaussian distribution, and then used a rotation matrix to transform the data. This results in elliptical data that is not aligned with a standard basis.

**Generating the Data**

```{r}
# Number of observations per class
n <- 1000

# Class Red
red.mu <- c(10, 20)
red.sd <- c(3, 5)

red <- matrix(c(rnorm(n, 0, red.sd[1]),
                rnorm(n, 0, red.sd[2])), 
              nrow = n, byrow = F) %>%
  rotate(-pi/6)
red[,1] <- red[,1] + red.mu[1]
red[,2] <- red[,2] + red.mu[2]


red.cov <- cov_mat(red)
red.mu <- mu_mat(red)
red.tbl <- tibble(u = red[,1],
                  v = red[,2],
                  class = "red")

# Class Green
green.mu <- c(5, 7)
green.sd <- c(3, 7)

green <- matrix(c(rnorm(n, 0, green.sd[1]),
                  rnorm(n, 0, green.sd[2])), 
                nrow = n, byrow = F) %>%
  rotate(pi/5)
green[,1] <- green[,1] + green.mu[1]
green[,2] <- green[,2] + green.mu[2]

green.cov <- cov_mat(green)
green.mu <- mu_mat(green)
green.tbl <- tibble(u = green[,1],
                    v = green[,2],
                    class = "green")
```

```{r, fig.width = 8, fig.height = 4, dpi=300, echo = FALSE}
bind_rows(red.tbl, green.tbl) %>%
  mutate_at(vars(class), as.factor) %>%
  ggplot(aes(x = u, y = v, color = class)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("green", "red")) +
  labs(title = "Two classes with known mean and covariance")
```

To calculate the Fisher's linear discriminant, we can use the formula

$$\vec{w} \varpropto (\Sigma_0 + \Sigma_1)^{-1} (\vec{\mu_1} - \vec{\mu_0} )$$

which gives us the direction of the best vector to project our data onto.

```{r}
cov_sum <- red.cov + green.cov
w <- solve(cov_sum) %*% (red.mu - green.mu)
w <- w / as.numeric(sqrt(t(w) %*% w))  # Normalize the vector
```

```{r, echo = FALSE}
proj_red_w <- proj(red, w)
proj_green_w <- proj(green, w)

prw.mu <- mean(proj_red_w[,1])
prw.sd <- sd(proj_red_w[,1])
pgw.mu <- mean(proj_green_w[,1])
pgw.sd <- sd(proj_green_w[,1])

c <- root_bisect(function(x){dnorm(x, prw.mu, prw.sd) - dnorm(x, pgw.mu, pgw.sd)},
                 lower = prw.mu, upper = pgw.mu)
c.prime <- c*(w[1]^2 + w[2]^2) / (w[1] * w[2])
```


```{r, fig.width = 4, fig.height = 6, dpi=300, echo = FALSE}
p1 <- bind_rows(red.tbl, green.tbl) %>%
  mutate_at(vars(class), as.factor) %>%
  ggplot(aes(x = u, y = v, color = class)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = -w[1] / w[2], intercept = c.prime,
              linetype = "dashed") +
  geom_abline(slope = w[2] / w[1], intercept = 0) +
  scale_color_manual(values = c("green", "red")) +
  coord_fixed()
```

```{r, fig.width = 8, fig.height = 6, dpi=300, echo = FALSE}
p2 <- bind_rows(tibble(proj = proj_red_w[,1], class = "red"), 
          tibble(proj = proj_green_w[,1], class = "green")) %>%
  mutate_at(vars(class), as.factor) %>%
  ggplot(aes(x = proj, y = ..density.., fill = class, color = class)) +
  geom_histogram(alpha = 0.3, position = "identity", bins = 35) +
  geom_vline(xintercept = c, linetype = "dashed") + 
  scale_fill_manual(values = c("green", "red")) +
  scale_color_manual(values = c("green", "red"))
```

The optimal separating hyperplane was found by estimating the normal distributions of the projected classes, and using a root finding function to find the point where the two distributions are equal.

The linear regression model that we can use to do classification is

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

where $y$ is either 0 (green class) or 1 (red class). If we let $y = 1/2$ be our threshold for classification, then our best prediction for a new observation, $x$, is

$$
\begin{cases}
x\in k_0 & \text{if } \beta_0 + \beta_1 x_1 + \beta_2 x_2 \le \frac{1}{2} \\
x\in k_1 & \text{if } \beta_0 + \beta_1 x_1 + \beta_2 x_2 > \frac{1}{2} \\
\end{cases}
$$

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
gridExtra::grid.arrange(p1, p2, nrow = 1, 
                        top = "Fisher Criterion and Optimal Projection\n")
```

```{r, echo = FALSE}
# Class Green == 1
# Class Red   == 0
lin.tbl <- bind_rows(tibble(X1 = green[,1],
                            X2 = green[,2],
                            Y  = 0),
                     tibble(X1 = red[,1],
                            X2 = red[,2],
                            Y  = 1))

model_fit <- glm(Y ~ X1 + X2, data = lin.tbl, family = "binomial")
B <- model_fit$coefficients

logi <- data.frame(intercept = -B[1]/B[3],
                   slope     = -B[2]/B[3],
                   Boundary      = "Logistic")

fish <- data.frame(intercept = c.prime,
                   slope     = -w[1] / w[2],
                   Boundary      = "Fisher")

lin_fit <- function(X, y) {
  X <- cbind(matrix(1, nrow(X), 1), X)
  
  solve((t(X) %*% X)) %*% (t(X) %*% y)
}

X.lin <- select(lin.tbl, X1, X2) %>% as.matrix
y.lin <- lin.tbl$Y %>% as.matrix()
z <- lin_fit(X.lin, y.lin)
linear <- data.frame(intercept = (1 - 2*z[1]) / (2*z[3]),
                     slope     = -z[2] / z[3],
                     Boundary  = "Linear")
```

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
bind_rows(red.tbl, green.tbl) %>%
  mutate_at(vars(class), as.factor) %>%
  ggplot(aes(x = u, y = v, color = class)) +
  geom_point(alpha = 0.3) +
  geom_abline(data = fish, 
              aes(slope = slope, intercept = intercept, linetype = Boundary)) +
  geom_abline(data = linear,
              aes(slope = slope, intercept = intercept, linetype = Boundary)) +
  scale_color_manual(name = "Class", values = c("green", "red")) +
  labs(title = "Decision Boundary",
       subtitle = "Linear Regression v. Fisher Criterion")
```
